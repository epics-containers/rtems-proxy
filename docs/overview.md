# RTEMS Proxy

## Overview

At DLS we are moving to running EPICS IOCs in Kubernetes. We are also transitioning all our VME crates from VxWorks to RTEMS.

Linux soft IOCs natively run as pods in Kubernetes, but for RTEMS IOCs we need a proxy container to control and monitor the RTEMS IOC running in a VME crate.

This project implements that proxy container.

The proxy container performs the following functions implemented in Python:

- Uses telnet and pexpect to control the RTEMS IOC running on the VME Crate.
- Configure the Crate by dropping into motboot and setting nvm variables.
- Start and stop the IOC by sending commands to the telnet session.
- Monitor the IOC by connecting the telnet session to the stdio of the container.
- Manage the IOC runtime assets:
    - Copy the IOC binary to a TFTP server for initial booting of the RTEMS target.
    - Copy runtime assets to an NFSv2 server for the RTEMS target to access at runtime.
      - startup script
      - EPICS database file
      - Stream device protocol files

## Why a proxy container?

Having a proxy container means that building and managing RTEMS IOCs can have an almost identical workflow to Linux soft IOCs.

- Starting and stopping IOCs can be done in the same way
- Building and deploying new versions of the IOC can be done in the same way
- Logging is identical as the stdout of the proxy container provides the logs, just like the stdout of a Linux soft IOC container.
- The proxy has even been designed to support working in a developer container to debug and test the RTEMS IOC in a local development environment, just like a Linux soft IOC.

## 3 Flavours of RTEMS IOC

As we transition there are 3 ways in which we will be deploying RTEMS IOCs.

For information on epics-containers, see the [epics-containers documentation](https://epics-containers.github.io/). For the DLS specific implementation details see the [DLS documentation](https://dev-guide.diamond.ac.uk/epics-containers/).

### 1. Fully Containerised with ibek

The end goal is that all IOCs will be done in this way.

The RTEMS IOC is cross compiled inside of the container build and becomes a part of the [Generic IOC](https://epics-containers.github.io/main/explanations/introduction.html#generic-iocs-and-instances) container image.

This container build happens in CI and the resulting container image is published to a registry. This is exactly the same workflow as Linux soft IOCs.

For build time:

- define the generic IOC exactly like you do for a Linux Generic IOC
- that means use the ansible.sh script to fetch and build the required support modules
- the only difference is that ARCH is set to RTEMS-beatnik in the CI build e.g. see [ioc-dlsvmevac CI](https://gitlab.diamond.ac.uk/controls/containers/iocs/ioc-dlsvmevac/-/blob/2025.5.1/.gitlab-ci.yml?ref_type=tags#L41-43)
- when you tag a release a container image will be built and published just like a Linux Generic IOC's

At runtime:

- the binary is extracted from the container and made available to the RTEMS target via a TFTP server.
- protocol files are also extracted from the container and made available to the RTEMS target via an NFSv2 server.
- the startup script and database are generated by ibek from an `ioc.yaml` instance description file and made available to the RTEMS target via the NFSv2 server.
- the proxy container uses telnet to:
  - drop into motboot on the RTEMS target and set nvm variables to point to the correct TFTP and NFS servers
  - boots the IOC.
  - links the telnet session to the container stdio for logging and attaching to the IOC shell.

Example Generic IOC: [ioc-dlsvmevac](https://gitlab.diamond.ac.uk/controls/containers/iocs/ioc-dlsvmevac/-/tree/2025.5.1?ref_type=tags)

Example IOC instance: [bl04i-va-ioc-01](https://gitlab.diamond.ac.uk/controls/containers/beamline/i04-services/-/tree/main/services/bl04i-va-ioc-01)

**Status**: This approach is proven and the above example has been tested on I04. However this was implemented using RTEMS6 and for the moment we are targetting RTEMS5. For the RTEMS5 rollout we will use one of the two following interim approaches.

### 2. Containerised Proxy with Manual RTEMS IOC Management

This is an interim step for non-builder VxWorks IOCs.

In this case the VxWorks IOC is manually ported to RTEMS and released to /dls_sw/prod by the build server.

The changes required are:
- update to EPICS v7
- change the src/Makefile target e.g.: `PROD_IOC_RTEMS = BL06I-MO-IOC-02`
- make sure you are using support module versions/branches that support RTEMS5
- rewrite the startup script to be compatible with RTEMS5
- release to prod using `dls-release`
- for more details on the porting process see:
  - confluence: https://confluence.diamond.ac.uk/display/CNTRLS/RTEMS
  - slack channel: #rtems6
  - Accelerator Team
  - Pete Leciester

Once you have a release you can deploy a proxy container to manage your RTEMS IOC. See the example proxy instance below.

Example IOC instance: [bl06i-mo-ioc-02](https://gitlab.diamond.ac.uk/controls/containers/iocs/ioc-bl06i-mo-02)

Example RTEMS proxy instance: [bl06i-mo-ioc-02](https://gitlab.diamond.ac.uk/controls/containers/beamline/i06-services/-/tree/main/services/bl06i-mo-ioc-02)

## 3. Build Server and ibek

This hybrid approach will be useful for quickly getting up and running on RTEMS 5 if your existing IOC uses XmlBuilder.

- It will use a Generic binary that is pre-built for e.g. beamline vacuum
- It will use an auto-converted ioc.yaml to get ibek to generate the startup script and database
- It will still pull all the DB templates from /dls_sw/prod

This approach is discussed in more detail here: [RTEMS Hybrid IOCs](hybrid.md)
